{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:47:23.765351700Z",
     "start_time": "2023-11-26T23:47:04.619446600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\dev\\envs\\hakiko\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\dev\\envs\\hakiko\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting pytorchvideo\n",
      "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
      "     ---------------------------------------- 0.0/132.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 132.7/132.7 kB 3.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers in d:\\dev\\envs\\hakiko\\lib\\site-packages (4.35.2)\n",
      "Collecting fvcore (from pytorchvideo)\n",
      "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "     ---------------------------------------- 0.0/50.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.2/50.2 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: av in d:\\dev\\envs\\hakiko\\lib\\site-packages (from pytorchvideo) (11.0.0)\n",
      "Collecting parameterized (from pytorchvideo)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting iopath (from pytorchvideo)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "     ---------------------------------------- 0.0/42.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.2/42.2 kB 2.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: networkx in d:\\dev\\envs\\hakiko\\lib\\site-packages (from pytorchvideo) (3.1)\n",
      "Requirement already satisfied: filelock in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in d:\\dev\\envs\\hakiko\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: termcolor>=1.1 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from fvcore->pytorchvideo) (2.3.0)\n",
      "Requirement already satisfied: Pillow in d:\\dev\\envs\\hakiko\\lib\\site-packages (from fvcore->pytorchvideo) (9.4.0)\n",
      "Collecting tabulate (from fvcore->pytorchvideo)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting portalocker (from iopath->pytorchvideo)\n",
      "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl.metadata\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\envs\\hakiko\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\keret\\appdata\\roaming\\python\\python310\\site-packages (from portalocker->iopath->pytorchvideo) (306)\n",
      "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
      "  Building wheel for pytorchvideo (setup.py): started\n",
      "  Building wheel for pytorchvideo (setup.py): finished with status 'done'\n",
      "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188704 sha256=dd52c5fcc7adb5d42b7ee554ed1a9aea903ebf93344197f08cfa8ab7942b187e\n",
      "  Stored in directory: c:\\users\\keret\\appdata\\local\\pip\\cache\\wheels\\ff\\4e\\81\\0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n",
      "  Building wheel for fvcore (setup.py): started\n",
      "  Building wheel for fvcore (setup.py): finished with status 'done'\n",
      "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61417 sha256=f7b3dc36c24c880a90f1cb9f01b3e6ec8a59b69ac081a172569b319841528346\n",
      "  Stored in directory: c:\\users\\keret\\appdata\\local\\pip\\cache\\wheels\\01\\c0\\af\\77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
      "  Building wheel for iopath (setup.py): started\n",
      "  Building wheel for iopath (setup.py): finished with status 'done'\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31542 sha256=3e8bb5ecbc9ff0ac57160c3797c99ddb30854c4946562fc48fb7c81b84cd879e\n",
      "  Stored in directory: c:\\users\\keret\\appdata\\local\\pip\\cache\\wheels\\9a\\a3\\b6\\ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built pytorchvideo fvcore iopath\n",
      "Installing collected packages: yacs, tabulate, portalocker, parameterized, iopath, fvcore, pytorchvideo\n",
      "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-2.8.2 pytorchvideo-0.1.5 tabulate-0.9.0 yacs-0.1.8\n",
      "Requirement already satisfied: av in d:\\dev\\envs\\hakiko\\lib\\site-packages (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorchvideo transformers\n",
    "!pip install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import av"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:18:14.527780300Z",
     "start_time": "2023-11-26T23:18:14.358183200Z"
    }
   },
   "id": "8309036b7cdbefc7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a923578fe54c44089eb34487b1589750"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87877e32e7394639adc622f96d975796"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/377M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54355034e9ed4931b3344a8cbfc093c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.538973870330655, 4.226492976822624] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m processor \u001B[38;5;241m=\u001B[39m VideoMAEImageProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMCG-NJU/videomae-base\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m model \u001B[38;5;241m=\u001B[39m VideoMAEForPreTraining\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMCG-NJU/videomae-base\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m pixel_values \u001B[38;5;241m=\u001B[39m \u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mpixel_values\n\u001B[0;32m     13\u001B[0m num_patches_per_frame \u001B[38;5;241m=\u001B[39m (model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mimage_size \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpatch_size) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     14\u001B[0m seq_length \u001B[38;5;241m=\u001B[39m (num_frames \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mtubelet_size) \u001B[38;5;241m*\u001B[39m num_patches_per_frame\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\image_processing_utils.py:549\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[1;34m(self, images, **kwargs)\u001B[0m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[0;32m    548\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 549\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:320\u001B[0m, in \u001B[0;36mVideoMAEImageProcessor.preprocess\u001B[1;34m(self, videos, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m videos \u001B[38;5;241m=\u001B[39m make_batched(videos)\n\u001B[1;32m--> 320\u001B[0m videos \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    321\u001B[0m     [\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_image(\n\u001B[0;32m    323\u001B[0m             image\u001B[38;5;241m=\u001B[39mimg,\n\u001B[0;32m    324\u001B[0m             do_resize\u001B[38;5;241m=\u001B[39mdo_resize,\n\u001B[0;32m    325\u001B[0m             size\u001B[38;5;241m=\u001B[39msize,\n\u001B[0;32m    326\u001B[0m             resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    327\u001B[0m             do_center_crop\u001B[38;5;241m=\u001B[39mdo_center_crop,\n\u001B[0;32m    328\u001B[0m             crop_size\u001B[38;5;241m=\u001B[39mcrop_size,\n\u001B[0;32m    329\u001B[0m             do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    330\u001B[0m             rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[0;32m    331\u001B[0m             do_normalize\u001B[38;5;241m=\u001B[39mdo_normalize,\n\u001B[0;32m    332\u001B[0m             image_mean\u001B[38;5;241m=\u001B[39mimage_mean,\n\u001B[0;32m    333\u001B[0m             image_std\u001B[38;5;241m=\u001B[39mimage_std,\n\u001B[0;32m    334\u001B[0m             data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    335\u001B[0m             input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    336\u001B[0m         )\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m video\n\u001B[0;32m    338\u001B[0m     ]\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m videos\n\u001B[0;32m    340\u001B[0m ]\n\u001B[0;32m    342\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: videos}\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchFeature(data\u001B[38;5;241m=\u001B[39mdata, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:321\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m videos \u001B[38;5;241m=\u001B[39m make_batched(videos)\n\u001B[0;32m    320\u001B[0m videos \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 321\u001B[0m     [\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_image(\n\u001B[0;32m    323\u001B[0m             image\u001B[38;5;241m=\u001B[39mimg,\n\u001B[0;32m    324\u001B[0m             do_resize\u001B[38;5;241m=\u001B[39mdo_resize,\n\u001B[0;32m    325\u001B[0m             size\u001B[38;5;241m=\u001B[39msize,\n\u001B[0;32m    326\u001B[0m             resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    327\u001B[0m             do_center_crop\u001B[38;5;241m=\u001B[39mdo_center_crop,\n\u001B[0;32m    328\u001B[0m             crop_size\u001B[38;5;241m=\u001B[39mcrop_size,\n\u001B[0;32m    329\u001B[0m             do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    330\u001B[0m             rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[0;32m    331\u001B[0m             do_normalize\u001B[38;5;241m=\u001B[39mdo_normalize,\n\u001B[0;32m    332\u001B[0m             image_mean\u001B[38;5;241m=\u001B[39mimage_mean,\n\u001B[0;32m    333\u001B[0m             image_std\u001B[38;5;241m=\u001B[39mimage_std,\n\u001B[0;32m    334\u001B[0m             data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    335\u001B[0m             input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    336\u001B[0m         )\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m video\n\u001B[0;32m    338\u001B[0m     ]\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m videos\n\u001B[0;32m    340\u001B[0m ]\n\u001B[0;32m    342\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: videos}\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchFeature(data\u001B[38;5;241m=\u001B[39mdata, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:322\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m videos \u001B[38;5;241m=\u001B[39m make_batched(videos)\n\u001B[0;32m    320\u001B[0m videos \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    321\u001B[0m     [\n\u001B[1;32m--> 322\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_preprocess_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_resize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_resize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m            \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_center_crop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_center_crop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    328\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcrop_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcrop_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_rescale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_rescale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrescale_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrescale_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_normalize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage_mean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage_std\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_data_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_data_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m video\n\u001B[0;32m    338\u001B[0m     ]\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m videos\n\u001B[0;32m    340\u001B[0m ]\n\u001B[0;32m    342\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: videos}\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchFeature(data\u001B[38;5;241m=\u001B[39mdata, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:219\u001B[0m, in \u001B[0;36mVideoMAEImageProcessor._preprocess_image\u001B[1;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001B[0m\n\u001B[0;32m    216\u001B[0m     input_data_format \u001B[38;5;241m=\u001B[39m infer_channel_dimension_format(image)\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_resize:\n\u001B[1;32m--> 219\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_data_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_data_format\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_center_crop:\n\u001B[0;32m    222\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcenter_crop(image, size\u001B[38;5;241m=\u001B[39mcrop_size, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:168\u001B[0m, in \u001B[0;36mVideoMAEImageProcessor.resize\u001B[1;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize must have \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshortest_edge\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m as keys. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resize(\n\u001B[0;32m    169\u001B[0m     image,\n\u001B[0;32m    170\u001B[0m     size\u001B[38;5;241m=\u001B[39moutput_size,\n\u001B[0;32m    171\u001B[0m     resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    172\u001B[0m     data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    173\u001B[0m     input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    175\u001B[0m )\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\image_transforms.py:326\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001B[0m\n\u001B[0;32m    324\u001B[0m do_rescale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(image, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[1;32m--> 326\u001B[0m     do_rescale \u001B[38;5;241m=\u001B[39m \u001B[43m_rescale_for_pil_conversion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    327\u001B[0m     image \u001B[38;5;241m=\u001B[39m to_pil_image(image, do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n\u001B[0;32m    328\u001B[0m height, width \u001B[38;5;241m=\u001B[39m size\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\image_transforms.py:150\u001B[0m, in \u001B[0;36m_rescale_for_pil_conversion\u001B[1;34m(image)\u001B[0m\n\u001B[0;32m    148\u001B[0m     do_rescale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    151\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    152\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgot [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage\u001B[38;5;241m.\u001B[39mmin()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] which cannot be converted to uint8.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    153\u001B[0m     )\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m do_rescale\n",
      "\u001B[1;31mValueError\u001B[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.538973870330655, 4.226492976822624] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "num_frames = 16\n",
    "video = list(np.random.randn(16, 3, 224, 224))\n",
    "\n",
    "processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "pixel_values = processor(video, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n",
    "seq_length = (num_frames // model.config.tubelet_size) * num_patches_per_frame\n",
    "bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n",
    "\n",
    "outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "loss = outputs.loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:17:27.016842300Z",
     "start_time": "2023-11-26T23:16:39.579367100Z"
    }
   },
   "id": "9e60ad61c062f422"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading eating_spaghetti.mp4:   0%|          | 0.00/1.01M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e7a1d560970405093136efea7323b3e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "file_path = hf_hub_download(\n",
    "\n",
    "    repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n",
    "\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:17:37.817654900Z",
     "start_time": "2023-11-26T23:17:36.836178600Z"
    }
   },
   "id": "b2fa8d03878b89c0"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\n",
    "def normalize(x):\n",
    "    min_x = np.min(x, axis=0)\n",
    "    max_x = np.max(x, axis=0)\n",
    "    return (x - min_x) / (max_x - min_x)\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    print()\n",
    "    return np.stack([normalize(x.to_ndarray(format=\"rgb24\")) for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:31:43.066221600Z",
     "start_time": "2023-11-26T23:31:43.032706700Z"
    }
   },
   "id": "e8f49224b0ddde23"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "container = av.open(file_path)\n",
    "\n",
    "# sample 16 frames\n",
    "\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:31:43.315058900Z",
     "start_time": "2023-11-26T23:31:43.289530100Z"
    }
   },
   "id": "c004ddf97ddae993"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, VideoMAEModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:31:46.577605Z",
     "start_time": "2023-11-26T23:31:43.582497600Z"
    }
   },
   "id": "98861803a53c1f9d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.538973870330655, 4.226492976822624] which cannot be converted to uint8.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# prepare video for the model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[43mimage_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvideo\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# forward pass\u001B[39;00m\n\u001B[0;32m      4\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\image_processing_utils.py:549\u001B[0m, in \u001B[0;36mBaseImageProcessor.__call__\u001B[1;34m(self, images, **kwargs)\u001B[0m\n\u001B[0;32m    547\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BatchFeature:\n\u001B[0;32m    548\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 549\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:320\u001B[0m, in \u001B[0;36mVideoMAEImageProcessor.preprocess\u001B[1;34m(self, videos, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m videos \u001B[38;5;241m=\u001B[39m make_batched(videos)\n\u001B[1;32m--> 320\u001B[0m videos \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    321\u001B[0m     [\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_image(\n\u001B[0;32m    323\u001B[0m             image\u001B[38;5;241m=\u001B[39mimg,\n\u001B[0;32m    324\u001B[0m             do_resize\u001B[38;5;241m=\u001B[39mdo_resize,\n\u001B[0;32m    325\u001B[0m             size\u001B[38;5;241m=\u001B[39msize,\n\u001B[0;32m    326\u001B[0m             resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    327\u001B[0m             do_center_crop\u001B[38;5;241m=\u001B[39mdo_center_crop,\n\u001B[0;32m    328\u001B[0m             crop_size\u001B[38;5;241m=\u001B[39mcrop_size,\n\u001B[0;32m    329\u001B[0m             do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    330\u001B[0m             rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[0;32m    331\u001B[0m             do_normalize\u001B[38;5;241m=\u001B[39mdo_normalize,\n\u001B[0;32m    332\u001B[0m             image_mean\u001B[38;5;241m=\u001B[39mimage_mean,\n\u001B[0;32m    333\u001B[0m             image_std\u001B[38;5;241m=\u001B[39mimage_std,\n\u001B[0;32m    334\u001B[0m             data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    335\u001B[0m             input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    336\u001B[0m         )\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m video\n\u001B[0;32m    338\u001B[0m     ]\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m videos\n\u001B[0;32m    340\u001B[0m ]\n\u001B[0;32m    342\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: videos}\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchFeature(data\u001B[38;5;241m=\u001B[39mdata, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:321\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m videos \u001B[38;5;241m=\u001B[39m make_batched(videos)\n\u001B[0;32m    320\u001B[0m videos \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 321\u001B[0m     [\n\u001B[0;32m    322\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_image(\n\u001B[0;32m    323\u001B[0m             image\u001B[38;5;241m=\u001B[39mimg,\n\u001B[0;32m    324\u001B[0m             do_resize\u001B[38;5;241m=\u001B[39mdo_resize,\n\u001B[0;32m    325\u001B[0m             size\u001B[38;5;241m=\u001B[39msize,\n\u001B[0;32m    326\u001B[0m             resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    327\u001B[0m             do_center_crop\u001B[38;5;241m=\u001B[39mdo_center_crop,\n\u001B[0;32m    328\u001B[0m             crop_size\u001B[38;5;241m=\u001B[39mcrop_size,\n\u001B[0;32m    329\u001B[0m             do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale,\n\u001B[0;32m    330\u001B[0m             rescale_factor\u001B[38;5;241m=\u001B[39mrescale_factor,\n\u001B[0;32m    331\u001B[0m             do_normalize\u001B[38;5;241m=\u001B[39mdo_normalize,\n\u001B[0;32m    332\u001B[0m             image_mean\u001B[38;5;241m=\u001B[39mimage_mean,\n\u001B[0;32m    333\u001B[0m             image_std\u001B[38;5;241m=\u001B[39mimage_std,\n\u001B[0;32m    334\u001B[0m             data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    335\u001B[0m             input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    336\u001B[0m         )\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m video\n\u001B[0;32m    338\u001B[0m     ]\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m videos\n\u001B[0;32m    340\u001B[0m ]\n\u001B[0;32m    342\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: videos}\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchFeature(data\u001B[38;5;241m=\u001B[39mdata, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:322\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    314\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    315\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    316\u001B[0m     )\n\u001B[0;32m    318\u001B[0m videos \u001B[38;5;241m=\u001B[39m make_batched(videos)\n\u001B[0;32m    320\u001B[0m videos \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    321\u001B[0m     [\n\u001B[1;32m--> 322\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_preprocess_image\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    323\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    324\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_resize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_resize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[43m            \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    327\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_center_crop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_center_crop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    328\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcrop_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcrop_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_rescale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_rescale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrescale_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrescale_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    331\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdo_normalize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_normalize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    332\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage_mean\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_mean\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    333\u001B[0m \u001B[43m            \u001B[49m\u001B[43mimage_std\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_std\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    334\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    335\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_data_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_data_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    337\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m video\n\u001B[0;32m    338\u001B[0m     ]\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m video \u001B[38;5;129;01min\u001B[39;00m videos\n\u001B[0;32m    340\u001B[0m ]\n\u001B[0;32m    342\u001B[0m data \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m: videos}\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m BatchFeature(data\u001B[38;5;241m=\u001B[39mdata, tensor_type\u001B[38;5;241m=\u001B[39mreturn_tensors)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:219\u001B[0m, in \u001B[0;36mVideoMAEImageProcessor._preprocess_image\u001B[1;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001B[0m\n\u001B[0;32m    216\u001B[0m     input_data_format \u001B[38;5;241m=\u001B[39m infer_channel_dimension_format(image)\n\u001B[0;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_resize:\n\u001B[1;32m--> 219\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_data_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_data_format\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_center_crop:\n\u001B[0;32m    222\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcenter_crop(image, size\u001B[38;5;241m=\u001B[39mcrop_size, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\models\\videomae\\image_processing_videomae.py:168\u001B[0m, in \u001B[0;36mVideoMAEImageProcessor.resize\u001B[1;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001B[0m\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSize must have \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheight\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshortest_edge\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m as keys. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;241m.\u001B[39mkeys()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resize(\n\u001B[0;32m    169\u001B[0m     image,\n\u001B[0;32m    170\u001B[0m     size\u001B[38;5;241m=\u001B[39moutput_size,\n\u001B[0;32m    171\u001B[0m     resample\u001B[38;5;241m=\u001B[39mresample,\n\u001B[0;32m    172\u001B[0m     data_format\u001B[38;5;241m=\u001B[39mdata_format,\n\u001B[0;32m    173\u001B[0m     input_data_format\u001B[38;5;241m=\u001B[39minput_data_format,\n\u001B[0;32m    174\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    175\u001B[0m )\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\image_transforms.py:326\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001B[0m\n\u001B[0;32m    324\u001B[0m do_rescale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(image, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage):\n\u001B[1;32m--> 326\u001B[0m     do_rescale \u001B[38;5;241m=\u001B[39m \u001B[43m_rescale_for_pil_conversion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    327\u001B[0m     image \u001B[38;5;241m=\u001B[39m to_pil_image(image, do_rescale\u001B[38;5;241m=\u001B[39mdo_rescale, input_data_format\u001B[38;5;241m=\u001B[39minput_data_format)\n\u001B[0;32m    328\u001B[0m height, width \u001B[38;5;241m=\u001B[39m size\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\transformers\\image_transforms.py:150\u001B[0m, in \u001B[0;36m_rescale_for_pil_conversion\u001B[1;34m(image)\u001B[0m\n\u001B[0;32m    148\u001B[0m     do_rescale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    151\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe image to be converted to a PIL image contains values outside the range [0, 1], \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    152\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgot [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage\u001B[38;5;241m.\u001B[39mmin()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage\u001B[38;5;241m.\u001B[39mmax()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] which cannot be converted to uint8.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    153\u001B[0m     )\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m do_rescale\n",
      "\u001B[1;31mValueError\u001B[0m: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.538973870330655, 4.226492976822624] which cannot be converted to uint8."
     ]
    }
   ],
   "source": [
    "# prepare video for the model\n",
    "inputs = image_processor(list(video), return_tensors=\"pt\")\n",
    "# forward pass\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:31:46.758402700Z",
     "start_time": "2023-11-26T23:31:46.578605100Z"
    }
   },
   "id": "b80e769ac9958984"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "video"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T23:31:09.165177600Z",
     "start_time": "2023-11-26T23:31:09.152671800Z"
    }
   },
   "id": "c9ca8e7f2360fc50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "16a6df97309edc03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
