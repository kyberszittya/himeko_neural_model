{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T19:39:31.567139Z",
     "start_time": "2024-12-10T19:39:23.458253Z"
    }
   },
   "outputs": [],
   "source": [
    "import ultralytics as ul\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Segmentation\n",
    "model = ul.SAM(\"sam2.1_b.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T19:39:32.641284300Z",
     "start_time": "2024-12-10T19:39:31.570387200Z"
    }
   },
   "id": "368b918d398594dd"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 8.00 GiB total capacity; 1.96 GiB already allocated; 4.79 GiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcattle_02.png\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# results list\u001B[39;00m\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mstyle\u001B[38;5;241m.\u001B[39muse(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdark_background\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Visualize the results\u001B[39;00m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\model.py:137\u001B[0m, in \u001B[0;36mSAM.__call__\u001B[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, source\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, bboxes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, points\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;124;03m    Performs segmentation prediction on the given image or video source.\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        >>> print(f\"Detected {len(results[0].masks)} masks\")\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(source, stream, bboxes, points, labels, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\model.py:111\u001B[0m, in \u001B[0;36mSAM.predict\u001B[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001B[0m\n\u001B[0;32m    109\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m    110\u001B[0m prompts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(bboxes\u001B[38;5;241m=\u001B[39mbboxes, points\u001B[38;5;241m=\u001B[39mpoints, labels\u001B[38;5;241m=\u001B[39mlabels)\n\u001B[1;32m--> 111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mpredict(source, stream, prompts\u001B[38;5;241m=\u001B[39mprompts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\engine\\model.py:554\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[1;32m--> 554\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\engine\\predictor.py:169\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[1;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m---> 35\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\engine\\predictor.py:255\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[1;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# Inference\u001B[39;00m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 255\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference(im, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39membed:\n\u001B[0;32m    257\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m [preds] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(preds, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m preds  \u001B[38;5;66;03m# yield embedding tensors\u001B[39;00m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:202\u001B[0m, in \u001B[0;36mPredictor.inference\u001B[1;34m(self, im, bboxes, points, labels, masks, multimask_output, *args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompts\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m, labels)\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(i \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m [bboxes, points, masks]):\n\u001B[1;32m--> 202\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(im, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompt_inference(im, bboxes, points, labels, masks, multimask_output)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:361\u001B[0m, in \u001B[0;36mPredictor.generate\u001B[1;34m(self, im, crop_n_layers, crop_overlap_ratio, crop_downscale_factor, point_grids, points_stride, points_batch_size, conf_thres, stability_score_thresh, stability_score_offset, crop_nms_thresh)\u001B[0m\n\u001B[0;32m    359\u001B[0m crop_masks, crop_scores, crop_bboxes \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (points,) \u001B[38;5;129;01min\u001B[39;00m batch_iterator(points_batch_size, points_for_image):\n\u001B[1;32m--> 361\u001B[0m     pred_mask, pred_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprompt_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcrop_im\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpoints\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;66;03m# Interpolate predicted masks to input size\u001B[39;00m\n\u001B[0;32m    363\u001B[0m     pred_mask \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39minterpolate(pred_mask[\u001B[38;5;28;01mNone\u001B[39;00m], (h, w), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, align_corners\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:726\u001B[0m, in \u001B[0;36mSAM2Predictor.prompt_inference\u001B[1;34m(self, im, bboxes, points, labels, masks, multimask_output, img_idx)\u001B[0m\n\u001B[0;32m    724\u001B[0m batched_mode \u001B[38;5;241m=\u001B[39m points \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m points[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# multi object prediction\u001B[39;00m\n\u001B[0;32m    725\u001B[0m high_res_features \u001B[38;5;241m=\u001B[39m [feat_level[img_idx]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m feat_level \u001B[38;5;129;01min\u001B[39;00m features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhigh_res_feats\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m--> 726\u001B[0m pred_masks, pred_scores, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msam_mask_decoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage_embed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mimg_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msam_prompt_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dense_pe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    729\u001B[0m \u001B[43m    \u001B[49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdense_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    731\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmultimask_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    732\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepeat_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    733\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhigh_res_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhigh_res_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;66;03m# (N, d, H, W) --> (N*d, H, W), (N, d) --> (N*d, )\u001B[39;00m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;66;03m# `d` could be 1 or 3 depends on `multimask_output`.\u001B[39;00m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pred_masks\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), pred_scores\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\decoders.py:353\u001B[0m, in \u001B[0;36mSAM2MaskDecoder.forward\u001B[1;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output, repeat_image, high_res_features)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    316\u001B[0m     image_embeddings: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    322\u001B[0m     high_res_features: Optional[List[torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    323\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    324\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;124;03m    Predicts masks given image and prompt embeddings.\u001B[39;00m\n\u001B[0;32m    326\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;124;03m        ... )\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m     masks, iou_pred, mask_tokens_out, object_score_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_masks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepeat_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepeat_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhigh_res_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhigh_res_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;66;03m# Select the correct mask or masks for output\u001B[39;00m\n\u001B[0;32m    363\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m multimask_output:\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\decoders.py:424\u001B[0m, in \u001B[0;36mSAM2MaskDecoder.predict_masks\u001B[1;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, repeat_image, high_res_features)\u001B[0m\n\u001B[0;32m    421\u001B[0m b, c, h, w \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    423\u001B[0m \u001B[38;5;66;03m# Run the transformer\u001B[39;00m\n\u001B[1;32m--> 424\u001B[0m hs, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_src\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    425\u001B[0m iou_token_out \u001B[38;5;241m=\u001B[39m hs[:, s, :]\n\u001B[0;32m    426\u001B[0m mask_tokens_out \u001B[38;5;241m=\u001B[39m hs[:, s \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m : (s \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_mask_tokens), :]\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\transformer.py:135\u001B[0m, in \u001B[0;36mTwoWayTransformer.forward\u001B[1;34m(self, image_embedding, image_pe, point_embedding)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# Apply transformer blocks and final layernorm\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 135\u001B[0m     queries, keys \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mqueries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoint_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;66;03m# Apply the final attention layer from the points to the image\u001B[39;00m\n\u001B[0;32m    143\u001B[0m q \u001B[38;5;241m=\u001B[39m queries \u001B[38;5;241m+\u001B[39m point_embedding\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\transformer.py:260\u001B[0m, in \u001B[0;36mTwoWayAttentionBlock.forward\u001B[1;34m(self, queries, keys, query_pe, key_pe)\u001B[0m\n\u001B[0;32m    258\u001B[0m attn_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcross_attn_image_to_token(q\u001B[38;5;241m=\u001B[39mk, k\u001B[38;5;241m=\u001B[39mq, v\u001B[38;5;241m=\u001B[39mqueries)\n\u001B[0;32m    259\u001B[0m keys \u001B[38;5;241m=\u001B[39m keys \u001B[38;5;241m+\u001B[39m attn_out\n\u001B[1;32m--> 260\u001B[0m keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeys\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m queries, keys\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:190\u001B[0m, in \u001B[0;36mLayerNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\functional.py:2515\u001B[0m, in \u001B[0;36mlayer_norm\u001B[1;34m(input, normalized_shape, weight, bias, eps)\u001B[0m\n\u001B[0;32m   2511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[0;32m   2512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m   2513\u001B[0m         layer_norm, (\u001B[38;5;28minput\u001B[39m, weight, bias), \u001B[38;5;28minput\u001B[39m, normalized_shape, weight\u001B[38;5;241m=\u001B[39mweight, bias\u001B[38;5;241m=\u001B[39mbias, eps\u001B[38;5;241m=\u001B[39meps\n\u001B[0;32m   2514\u001B[0m     )\n\u001B[1;32m-> 2515\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnormalized_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 8.00 GiB total capacity; 1.96 GiB already allocated; 4.79 GiB free; 2.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "results = model([\"cattle_02.png\"])  # results list\n",
    "plt.style.use('dark_background')\n",
    "# Visualize the results\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "\n",
    "    # Show results to screen (in supported environments)\n",
    "    plt.imshow(im_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T19:39:37.905567400Z",
     "start_time": "2024-12-10T19:39:32.646477100Z"
    }
   },
   "id": "bdb34c566b4e6091"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 8.00 GiB total capacity; 3.36 GiB already allocated; 3.39 GiB free; 3.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m pic \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokyo_02.png\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# results list\u001B[39;00m\n\u001B[0;32m      4\u001B[0m img \u001B[38;5;241m=\u001B[39m mpimg\u001B[38;5;241m.\u001B[39mimread(pic)\n\u001B[0;32m      6\u001B[0m plt\u001B[38;5;241m.\u001B[39mstyle\u001B[38;5;241m.\u001B[39muse(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdark_background\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\model.py:137\u001B[0m, in \u001B[0;36mSAM.__call__\u001B[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, source\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, bboxes\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, points\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;124;03m    Performs segmentation prediction on the given image or video source.\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;124;03m        >>> print(f\"Detected {len(results[0].masks)} masks\")\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(source, stream, bboxes, points, labels, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\model.py:111\u001B[0m, in \u001B[0;36mSAM.predict\u001B[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001B[0m\n\u001B[0;32m    109\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moverrides, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m    110\u001B[0m prompts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(bboxes\u001B[38;5;241m=\u001B[39mbboxes, points\u001B[38;5;241m=\u001B[39mpoints, labels\u001B[38;5;241m=\u001B[39mlabels)\n\u001B[1;32m--> 111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mpredict(source, stream, prompts\u001B[38;5;241m=\u001B[39mprompts, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\engine\\model.py:554\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[0;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[1;32m--> 554\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\engine\\predictor.py:169\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[1;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m---> 35\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\engine\\predictor.py:255\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[1;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# Inference\u001B[39;00m\n\u001B[0;32m    254\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 255\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference(im, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39membed:\n\u001B[0;32m    257\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m [preds] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(preds, torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;28;01melse\u001B[39;00m preds  \u001B[38;5;66;03m# yield embedding tensors\u001B[39;00m\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:202\u001B[0m, in \u001B[0;36mPredictor.inference\u001B[1;34m(self, im, bboxes, points, labels, masks, multimask_output, *args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompts\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m, labels)\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(i \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m [bboxes, points, masks]):\n\u001B[1;32m--> 202\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(im, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprompt_inference(im, bboxes, points, labels, masks, multimask_output)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:361\u001B[0m, in \u001B[0;36mPredictor.generate\u001B[1;34m(self, im, crop_n_layers, crop_overlap_ratio, crop_downscale_factor, point_grids, points_stride, points_batch_size, conf_thres, stability_score_thresh, stability_score_offset, crop_nms_thresh)\u001B[0m\n\u001B[0;32m    359\u001B[0m crop_masks, crop_scores, crop_bboxes \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[0;32m    360\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (points,) \u001B[38;5;129;01min\u001B[39;00m batch_iterator(points_batch_size, points_for_image):\n\u001B[1;32m--> 361\u001B[0m     pred_mask, pred_score \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprompt_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcrop_im\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpoints\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;66;03m# Interpolate predicted masks to input size\u001B[39;00m\n\u001B[0;32m    363\u001B[0m     pred_mask \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39minterpolate(pred_mask[\u001B[38;5;28;01mNone\u001B[39;00m], (h, w), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbilinear\u001B[39m\u001B[38;5;124m\"\u001B[39m, align_corners\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\predict.py:726\u001B[0m, in \u001B[0;36mSAM2Predictor.prompt_inference\u001B[1;34m(self, im, bboxes, points, labels, masks, multimask_output, img_idx)\u001B[0m\n\u001B[0;32m    724\u001B[0m batched_mode \u001B[38;5;241m=\u001B[39m points \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m points[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m  \u001B[38;5;66;03m# multi object prediction\u001B[39;00m\n\u001B[0;32m    725\u001B[0m high_res_features \u001B[38;5;241m=\u001B[39m [feat_level[img_idx]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m feat_level \u001B[38;5;129;01min\u001B[39;00m features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhigh_res_feats\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m--> 726\u001B[0m pred_masks, pred_scores, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msam_mask_decoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage_embed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mimg_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msam_prompt_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dense_pe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    729\u001B[0m \u001B[43m    \u001B[49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdense_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    731\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmultimask_output\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmultimask_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    732\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepeat_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    733\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhigh_res_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhigh_res_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    734\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    735\u001B[0m \u001B[38;5;66;03m# (N, d, H, W) --> (N*d, H, W), (N, d) --> (N*d, )\u001B[39;00m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;66;03m# `d` could be 1 or 3 depends on `multimask_output`.\u001B[39;00m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pred_masks\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m), pred_scores\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\decoders.py:353\u001B[0m, in \u001B[0;36mSAM2MaskDecoder.forward\u001B[1;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output, repeat_image, high_res_features)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    315\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    316\u001B[0m     image_embeddings: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    322\u001B[0m     high_res_features: Optional[List[torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    323\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    324\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;124;03m    Predicts masks given image and prompt embeddings.\u001B[39;00m\n\u001B[0;32m    326\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;124;03m        ... )\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m     masks, iou_pred, mask_tokens_out, object_score_logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_masks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m        \u001B[49m\u001B[43mimage_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse_prompt_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdense_prompt_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepeat_image\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepeat_image\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhigh_res_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhigh_res_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;66;03m# Select the correct mask or masks for output\u001B[39;00m\n\u001B[0;32m    363\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m multimask_output:\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\decoders.py:424\u001B[0m, in \u001B[0;36mSAM2MaskDecoder.predict_masks\u001B[1;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, repeat_image, high_res_features)\u001B[0m\n\u001B[0;32m    421\u001B[0m b, c, h, w \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    423\u001B[0m \u001B[38;5;66;03m# Run the transformer\u001B[39;00m\n\u001B[1;32m--> 424\u001B[0m hs, src \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_src\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    425\u001B[0m iou_token_out \u001B[38;5;241m=\u001B[39m hs[:, s, :]\n\u001B[0;32m    426\u001B[0m mask_tokens_out \u001B[38;5;241m=\u001B[39m hs[:, s \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m : (s \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_mask_tokens), :]\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\transformer.py:135\u001B[0m, in \u001B[0;36mTwoWayTransformer.forward\u001B[1;34m(self, image_embedding, image_pe, point_embedding)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;66;03m# Apply transformer blocks and final layernorm\u001B[39;00m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 135\u001B[0m     queries, keys \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mqueries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoint_embedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_pe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_pe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;66;03m# Apply the final attention layer from the points to the image\u001B[39;00m\n\u001B[0;32m    143\u001B[0m q \u001B[38;5;241m=\u001B[39m queries \u001B[38;5;241m+\u001B[39m point_embedding\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32md:\\dev\\envs\\hakiko\\lib\\site-packages\\ultralytics\\models\\sam\\modules\\transformer.py:245\u001B[0m, in \u001B[0;36mTwoWayAttentionBlock.forward\u001B[1;34m(self, queries, keys, query_pe, key_pe)\u001B[0m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;66;03m# Cross attention block, tokens attending to image embedding\u001B[39;00m\n\u001B[0;32m    244\u001B[0m q \u001B[38;5;241m=\u001B[39m queries \u001B[38;5;241m+\u001B[39m query_pe\n\u001B[1;32m--> 245\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[43mkeys\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkey_pe\u001B[49m\n\u001B[0;32m    246\u001B[0m attn_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcross_attn_token_to_image(q\u001B[38;5;241m=\u001B[39mq, k\u001B[38;5;241m=\u001B[39mk, v\u001B[38;5;241m=\u001B[39mkeys)\n\u001B[0;32m    247\u001B[0m queries \u001B[38;5;241m=\u001B[39m queries \u001B[38;5;241m+\u001B[39m attn_out\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 8.00 GiB total capacity; 3.36 GiB already allocated; 3.39 GiB free; 3.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "pic = \"tokyo_02.png\"\n",
    "results = model([pic]) # results list\n",
    "\n",
    "img = mpimg.imread(pic)\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "# Visualize the results\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(im_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T19:40:50.273096900Z",
     "start_time": "2024-12-10T19:40:49.377639300Z"
    }
   },
   "id": "f1a4c648af91f8b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4508defe6e213cef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
